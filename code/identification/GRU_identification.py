import argparse
import json
import os
import random
import numpy as np
from tqdm._tqdm import tqdm
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.optim as optim
import gc
import time
from tensorboardX import SummaryWriter

torch.backends.cudnn.deterministic = True
np.random.seed(1234)
torch.manual_seed(1234)
torch.cuda.manual_seed(1234)
torch.cuda.set_device(0)


class Model(nn.Module):
    def __init__(self, hidden_dim, vocab, embedding, dropout=0):
        super(Model, self).__init__()
        self.use_cuda = True if torch.cuda.is_available() else False
        self.hidden_dim = hidden_dim
        self.vocab = vocab
        self.emb = nn.Embedding(len(vocab['w2i']), 300)
        self.emb.weight.data = torch.from_numpy(embedding).float()
        self.gru = nn.GRU(300, hidden_dim)
        self.output = nn.Linear(hidden_dim*2, 2)
        self.dropout = nn.Dropout(p=dropout)

    def pad(self, sent):
        sent_len = [len(s) for s in sent]
        max_len = max(sent_len)
        for i in range(len(sent)):
            sent[i] = sent[i] + ['<pad>']*(max_len-len(sent[i]))

        return sent, sent_len

    def lookup_idx(self, sent):
        token_idxs = []
        for s in sent:
            token_idxs.append([self.vocab['w2i'][t] if t in self.vocab['w2i'] else self.vocab['w2i']['<unk>'] for t in s])

        return token_idxs

    def get_var(self, tensor):
        if self.use_cuda:
            return Variable(tensor.cuda())
        else:
            return Variable(tensor)

    def encode(self, sent):

        sent, sent_len = self.pad(sent)
        token_idx = self.lookup_idx(sent)
        token_idx = self.get_var(torch.LongTensor(token_idx))
        sent = self.emb(token_idx)

        batch_size, seq_len, emb_dim = sent.size()
        sent = torch.transpose(sent, 0, 1)
        if not isinstance(sent_len, torch.LongTensor):
            sent_len = torch.LongTensor(sent_len)
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len, enforce_sorted=False)
        sent_output, h_n = self.gru(sent_packed)
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]
        sent_output = torch.transpose(self.dropout(sent_output), 0, 1)

        return sent_output

    def forward(self, sent, relations):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
        """
        sent_output = self.encode(sent)

        rel_repr = []
        for para, rels in zip(sent_output, relations):
            e1, e2 = rels
            e1_idx = torch.arange(e1[0], e1[1])
            e2_idx = torch.arange(e2[0], e2[1])
            e1_repr = torch.max(para.index_select(0, self.get_var(e1_idx)), dim=0)[0]
            e2_repr = torch.max(para.index_select(0, self.get_var(e2_idx)), dim=0)[0]

            rel_repr.append(torch.cat((e1_repr, e2_repr)))

        rel_repr = torch.stack(rel_repr, dim=0)

        logits = self.output(rel_repr)

        return logits


def load_data(filename):

    def split(data):
        L = len(data)
        idx = np.random.permutation(L)
        train = [data[i] for i in idx[:L//10*6]]
        dev = [data[i] for i in idx[L//10*6:L//10*8]]
        test = [data[i] for i in idx[L//10*8:]]

        return train, dev, test

    pos_data = []
    neg_data = []
    with open(filename, "r") as fin:
        for line in fin:
            line_in = json.loads(line.strip())
            if line_in['label'] == 1:
                pos_data.append(line_in)
            else:
                neg_data.append(line_in)

    pos_train, pos_dev, pos_test = split(pos_data)
    neg_train, neg_dev, neg_test = split(neg_data)
    train = pos_train+neg_train
    dev = pos_dev+neg_dev
    test = pos_test+neg_test

    train = [train[i] for i in np.random.permutation(len(train))]
    dev = [dev[i] for i in np.random.permutation(len(dev))]
    test = [test[i] for i in np.random.permutation(len(test))]

    return {'train': train, 'dev': dev, 'test': test}


def generate_vocab(data):
    w2i, i2w = {}, {}

    special_tokens = ['<unk>', '<pad>']
    for t in special_tokens:
        i2w[len(w2i)] = t
        w2i[t] = len(w2i)

    for dd in data:
        for t in dd['sent'].split():
            if t not in w2i:
                w2i[t] = len(w2i)
                i2w[len(i2w)] = t

    return {'w2i': w2i, 'i2w': i2w}


def load_emb(vocab):
    print("Reading pre-trained embeddings")
    embeddings = np.random.normal(0, 0.01, (len(vocab['w2i']), 300))
    with open("/home/data/glove/glove.840B.300d.txt", "r") as embed_in:
        line_tqdm = tqdm(embed_in, dynamic_ncols=True)
        for idx, line in enumerate(line_tqdm):
            row = line.split()
            if len(row) != 301: continue
            if row[0] in vocab['w2i']:
                embeddings[vocab['w2i'][row[0]], :] = np.asarray([float(v) for v in row[1:]])
    embeddings[vocab['w2i']['<pad>']] = np.zeros((1, 300))

    return embeddings


def batchify(data, batch_size=64):
    batch_data = []
    tmp_sent, tmp_rel, tmp_label = [], [], []
    for instance in data:
        tmp_sent.append(instance['sent'].split())
        tmp_rel.append([instance['source']['idx'], instance['target']['idx']])
        tmp_label.append(instance['label'])
        if len(tmp_sent) == batch_size:
            batch_data.append([tmp_sent, tmp_rel, tmp_label])
            tmp_sent, tmp_rel, tmp_label = [], [], []

    if tmp_sent:
        batch_data.append([tmp_sent, tmp_rel, tmp_label])

    return batch_data


def evaluate(pred, label):
    tp, tn, fp, fn = 0, 0, 0, 0
    for p, l in zip(pred, label):
        if p == l:
            if p == 1:
                tp += 1
            else:
                tn += 1
        else:
            if p == 1:
                fp += 1
            else:
                fn += 1

    acc = (tp+tn)/len(pred)
    prec = tp/(tp+fp)
    rec = tp/(tp+fn)
    f1 = 2*prec*rec/(prec+rec)
    return acc, prec, rec, f1


def test(model, data, loss_function=None):
    model.eval()
    total_loss = []
    pred = []
    flattened_label = []
    for instance in data:
        sent, label = [instance['sent'].split()], [instance['label']]
        rel = [[instance['source']['idx'], instance['target']['idx']]]

        logits = model(sent, rel)
        if model.use_cuda:
            target_batch = Variable(torch.LongTensor(label)).cuda()
        else:
            target_batch = Variable(torch.LongTensor(label))

        if loss_function is not None:
            loss = loss_function(logits, target_batch)
            total_loss.extend(loss.data.cpu().numpy().tolist())
        pred.append(int(torch.argmax(logits, dim=-1).cpu()))
        flattened_label.append(instance['label'])

    acc, precision, recall, f1 = evaluate(pred, flattened_label)

    if loss_function is not None:
        print("\t\tLoss: {:0.5f}".format(sum(total_loss)/len(total_loss)))
    print("\t\tAccuracy: {:0.5f}".format(acc))
    print("\t\tPrecision: {:0.5f}".format(precision))
    print("\t\tRecall: {:0.5f}".format(recall))
    print("\t\tF1: {:0.5f}".format(f1))

    if loss_function is not None:
        return acc, precision, recall, f1, sum(total_loss)/len(total_loss)
    else:
        return acc, precision, recall, f1


def train(output_dir):

    out_dir = os.path.join(args.logdir, args.experiment)

    # setup tensorboard logging
    if args.tensorboard_logging:
        writer = SummaryWriter(out_dir)
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)

    # load data
    data = load_data('../data/peko_all.jsonl')
    del data['test']

    # generate vocab
    vocab = generate_vocab(data['train'])
    embedding = load_emb(vocab)

    batch_data = batchify(data['train'])

    model = Model(args.hidden_size, vocab, embedding)
    if model.use_cuda:
        model.cuda()

    labels = [d['label'] for d in data['train']]
    weight = torch.FloatTensor([sum(labels)/(len(labels)-sum(labels)), 1.])
    if model.use_cuda:
        weight = weight.cuda()
    loss_function = nn.CrossEntropyLoss(weight=weight, reduction='none')

    n_params = sum([np.prod(p.size()) for p in model.parameters()])
    print("#parameters: {}".format(n_params))

    parameters = model.parameters()
    optimizer = optim.Adam(parameters, lr=0.0001)

    dev_best = 0
    for epoch in range(1, args.epochs+1):
        total_loss = []
        batch_idxs = np.random.permutation(len(batch_data))
        model.train()
        print("Epoch {}:".format(epoch))
        start_time = time.time()
        for batch_idx in batch_idxs:

            model.zero_grad()
            sents, rels, labels = batch_data[batch_idx]

            logits = model(sents, rels)
            if model.use_cuda:
                target_batch = Variable(torch.LongTensor(labels)).cuda()
            else:
                target_batch = Variable(torch.LongTensor(labels))

            loss = loss_function(logits, target_batch)
            total_loss.extend(loss.data.cpu().numpy().tolist())
            loss.mean().backward()
            optimizer.step()
            gc.collect()
            torch.cuda.empty_cache()

        end_time = time.time()
        print("Time Elapsed: {:.3f}".format(end_time-start_time))
        if args.tensorboard_logging:
            writer.add_histogram("losses", np.asarray(total_loss), epoch, bins='auto')
            writer.add_scalar("TRAIN/loss", sum(total_loss)/len(total_loss), epoch)
        print("Train loss: {:05f}".format(sum(total_loss)/len(total_loss)))

        for set_info in ['train', 'dev']:
            print("\tTest on {}".format(set_info.upper()))
            with torch.no_grad():
                acc, precision, recall, f1, loss = test(model, data[set_info], loss_function)
            if args.tensorboard_logging:
                writer.add_scalar("{}/Accuracy".format(set_info.upper()), acc, epoch)
                writer.add_scalar("{}/Precision".format(set_info.upper()), precision, epoch)
                writer.add_scalar("{}/Recall".format(set_info.upper()), recall, epoch)
                writer.add_scalar("{}/F1".format(set_info.upper()), f1, epoch)
                if set_info == 'dev':
                    writer.add_scalar("{}/Loss".format(set_info.upper()), loss, epoch)

            if set_info == 'dev':
                if f1 > dev_best:
                    print("Save Model...\n")
                    torch.save(model, os.path.join(out_dir, 'gru_best_model.pt'))
                    best_acc = acc
                    best_precision = precision
                    best_recall = recall
                    dev_best = f1

    print("Best Result:")
    print("\tAccuracy: {:0.5f}".format(best_acc))
    print("\tPrecision: {:0.5f}".format(best_precision))
    print("\tRecall: {:0.5f}".format(best_recall))
    print("\tF1: {:0.5f}".format(dev_best))

    return


def model_test(model_file):

    data = load_data('../data/peko_all.jsonl')

    model = torch.load(model_file)

    for set_info in data.keys():
        print("\tTest on {}".format(set_info.upper()))
        with torch.no_grad():
            acc, precision, recall, f1 = test(model, data[set_info])

    return


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument('-s', '--seed', type=int, default=1234)
    parser.add_argument('-ep', '--epochs', type=int, default=50)
    parser.add_argument('-bs', '--batch_size', type=int, default=16)
    parser.add_argument('-hs', '--hidden_size', type=int, default=512)
    parser.add_argument('-lr', '--learning_rate', type=float, default=1e-6)

    parser.add_argument('-tb', '--tensorboard_logging', action='store_true')
    parser.add_argument('-log', '--logdir', type=str, default='/home/data/Precondition/')

    parser.add_argument('--load_model', type=str, default=None)
    parser.add_argument('-ex', '--experiment', type=str, default='test')
    parser.add_argument('--test', action='store_true')

    args = parser.parse_args()

    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed(args.seed)

    if args.test:
        model_test(args.load_model)
    else:
        train(args)
